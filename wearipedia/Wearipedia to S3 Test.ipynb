{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9992c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wearipedia\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import boto3\n",
    "import io\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ef5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../.env/.env')\n",
    "BUCKET_NAME = os.environ.get(\"WEARIPEDIA_S3_BUCKET_NAME\")\n",
    "PREFIX = \"wearipedia/raw/cronometer/\"\n",
    "BACKFILL = True\n",
    "email_address = os.getenv(f'CRONOMETER_EMAIL')\n",
    "password = os.getenv(f'CRONOMETER_PASSWORD')\n",
    "\n",
    "start_date='2025-04-23' #'2025-04-23' is first start date for logging. #@param {type:\"string\"}\n",
    "end_date=(datetime.datetime.today()- datetime.timedelta(days=2)).strftime('%Y-%m-%d') #@param {type:\"string\"}\n",
    "synthetic = False #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b61c2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful\n"
     ]
    }
   ],
   "source": [
    "# Get Device\n",
    "device = wearipedia.get_device(\"cronometer/cronometer\")\n",
    "\n",
    "if not synthetic:\n",
    "    device.authenticate({\"username\": email_address, \"password\": password})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8498b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_filenames(bucket_name: str, folder_prefix: str, region=\"us-east-2\"):\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "    \n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    result = []\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=folder_prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            result.append(obj[\"Key\"])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b864a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_dates_from_keys(keys: list[str]) -> set[str]:\n",
    "    date_pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")\n",
    "    dates = set()\n",
    "\n",
    "    for key in keys:\n",
    "        match = date_pattern.search(key)\n",
    "        if match:\n",
    "            dates.add(match.group(0))\n",
    "\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111e0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_dates(start_date: str, end_date: str, existing_dates: list[str]) -> list[str]:\n",
    "    # Convert to datetime\n",
    "    start = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    existing = set(existing_dates)\n",
    "\n",
    "    # Generate full date range\n",
    "    all_dates = {\n",
    "        (start + datetime.timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "        for i in range((end - start).days + 1)\n",
    "    }\n",
    "\n",
    "    # Find dates in range not in existing list\n",
    "    missing = sorted(all_dates - existing)\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa81f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_nan(obj):\n",
    "    if isinstance(obj, float) and math.isnan(obj):\n",
    "        return None\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: sanitize_nan(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [sanitize_nan(i) for i in obj]\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654b7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dicts_as_gzipped_jsonl_to_s3(records: list[dict], data_type: str, end_date: str, bucket: str, prefix: str, region: str = \"us-east-2\"):\n",
    "    buffer = io.BytesIO()\n",
    "\n",
    "    # Write DataFrame to GZIP CSV in memory\n",
    "    with gzip.GzipFile(fileobj=buffer, mode=\"w\") as gz:\n",
    "        for item in records:\n",
    "            sanitized = sanitize_nan(item)\n",
    "            line = json.dumps(sanitized) + \"\\n\"\n",
    "            gz.write(line.encode('utf-8'))\n",
    "\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Create timestamped filename\n",
    "    filename = f\"{data_type}_{end_date}.jsonl.gz\"\n",
    "    key = os.path.join(f\"{prefix}{end_date.replace('-','/')}\", filename)\n",
    "\n",
    "    # Upload to S3\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "    s3.upload_fileobj(buffer, bucket, key)\n",
    "\n",
    "    print(f\"Uploaded {filename} to S3 bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5401dead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2025-04-23', '2025-04-24', '2025-04-25', '2025-04-26', '2025-04-27', '2025-04-28', '2025-04-29', '2025-04-30', '2025-05-01', '2025-05-02', '2025-05-03', '2025-05-04', '2025-05-05', '2025-05-06', '2025-05-07', '2025-05-08', '2025-05-09', '2025-05-10', '2025-05-11', '2025-05-12', '2025-05-13', '2025-05-14', '2025-05-15', '2025-05-16', '2025-05-17', '2025-05-18', '2025-05-19', '2025-05-20', '2025-05-21', '2025-05-22', '2025-05-23', '2025-05-24', '2025-05-25', '2025-05-26', '2025-05-27', '2025-05-28', '2025-05-29', '2025-05-30', '2025-05-31', '2025-06-01', '2025-06-02', '2025-06-03', '2025-06-04', '2025-06-05', '2025-06-06', '2025-06-07', '2025-06-08', '2025-06-09', '2025-06-10', '2025-06-11', '2025-06-12', '2025-06-13', '2025-06-14', '2025-06-15', '2025-06-16', '2025-06-17', '2025-06-18', '2025-06-19', '2025-06-20', '2025-06-21', '2025-06-22', '2025-06-23', '2025-06-24', '2025-06-25', '2025-06-26', '2025-06-27', '2025-06-28', '2025-06-29', '2025-06-30', '2025-07-01', '2025-07-02', '2025-07-03', '2025-07-04', '2025-07-05', '2025-07-06', '2025-07-07', '2025-07-08', '2025-07-09', '2025-07-10', '2025-07-11', '2025-07-12', '2025-07-13']\n"
     ]
    }
   ],
   "source": [
    "bucket = BUCKET_NAME\n",
    "prefix = PREFIX\n",
    "\n",
    "files = list_s3_filenames(bucket, prefix)\n",
    "existing_dates = extract_unique_dates_from_keys(files)\n",
    "\n",
    "missing_dates = get_missing_dates(start_date, end_date, existing_dates)\n",
    "print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "452f0ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded dailySummary_2025-07-13.jsonl.gz to S3 bucket.\n",
      "Uploaded servings_2025-07-13.jsonl.gz to S3 bucket.\n",
      "Uploaded exercises_2025-07-13.jsonl.gz to S3 bucket.\n",
      "Uploaded biometrics_2025-07-13.jsonl.gz to S3 bucket.\n"
     ]
    }
   ],
   "source": [
    "for date in missing_dates:\n",
    "    if BACKFILL:\n",
    "        params = {\"start_date\": start_date, \"end_date\": end_date}\n",
    "        date = end_date\n",
    "    else:\n",
    "        params = {\"start_date\": date, \"end_date\": date}\n",
    "    datasets = ['dailySummary', 'servings', 'exercises', 'biometrics']\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            data = device.get_data(dataset, params=params)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        upload_dicts_as_gzipped_jsonl_to_s3(\n",
    "            data,\n",
    "            data_type=dataset,\n",
    "            end_date=date,\n",
    "            bucket=BUCKET_NAME,\n",
    "            prefix=PREFIX\n",
    "        )\n",
    "    if BACKFILL:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a599d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
